{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7042738b-b374-4757-b14a-252242b78998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5820051-1fcc-49c8-91a8-a59683ab415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV 파일 path 설정\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "os.chdir(r'C:\\Users\\SSAFY\\Desktop\\crawling\\hani')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf3b0b3-e6a1-42c4-ab1b-24a792d9be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역변수 선언\n",
    "article_data = []\n",
    "\n",
    "# 뉴스 카테고리\n",
    "news_category = ['politics','society', 'area', 'economy', 'international', 'culture', 'sports', 'science', 'animalpeople', 'opinion'] # 정치, 사회, 전국, 경제, 국제, 문화, 스포츠, 미래과학, 애니멀피플, 오피니언\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444e2055-a7dd-467c-b1fd-3b13d54f8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 카테고리별 url 만들기\n",
    "def make_hani_url(news_category_index, page):\n",
    "    default_url = 'https://www.hani.co.kr/arti/'\n",
    "    query_string = '?page='\n",
    "    default_url += news_category[news_category_index]\n",
    "    return default_url + query_string + str(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ef2ee5-0bd3-40a1-aeae-28e77c7dd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#2 카테고리 페이지 접근 후 링크 뽑아오기\n",
    "# 최대 15개\n",
    "def extract_hani_article_link(url):\n",
    "    header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(url, headers=header)\n",
    "    html = response.text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    a_elements = soup.select('li.ArticleList_item__jPMVW > article > a')\n",
    "    links = []\n",
    "    for element in a_elements:\n",
    "        link = \"https://www.hani.co.kr\" + element.get('href')\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c76fb8e-9572-4fcd-a7b5-f04553ae8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "# temp_url = 'https://www.hani.co.kr/arti/society/health/1130721.html'\n",
    "#3. 해당 기사 링크 접속 후 아티클 정보 불러오기\n",
    "def get_article(url):\n",
    "    header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(url, headers=header)\n",
    "    html = response.text\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    soup = bs(html, 'html.parser')\n",
    "    json_contents = soup.select('script#__NEXT_DATA__') # 아티클 Json에 해당하는 부분 가져오기\n",
    "    json_content = json.loads(json_contents[0].get_text(strip=True)) #Json 형식으로 변경\n",
    "    article = json_content.get('props', {}).get('pageProps', {}).get('article', {})\n",
    "    return article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5c91db-b3fa-4659-adc0-dfb96881ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 HTML 태그 제거\n",
    "import re\n",
    "def delete_html_tag(context):\n",
    "    processed_context = []\n",
    "    for text in context:\n",
    "        text = re.sub(r'<[%>]+Ws+(?=<)|<[^>]+>', '', text).strip()\n",
    "        if text:\n",
    "            processed_context.append(text)\n",
    "        return  processed_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62735233-75f4-480f-9fc9-d56939606969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 불러온 기사 정보에서 필요한 정보를 뽑아 news_data 배열에 저장\n",
    "def extract_article_info(url):\n",
    "    article = get_article(url)\n",
    "    \n",
    "    # 제목\n",
    "    title = article.get('title', {})\n",
    "\n",
    "    # 카테고리 대분류\n",
    "    section = article.get('section', {}).get('label', {})\n",
    "\n",
    "    # 카테고리 중분류\n",
    "    subSection = article.get('subSection', {}).get('label', {})\n",
    "\n",
    "    # 작성일자\n",
    "    createDate = article.get('createDate', {})\n",
    "\n",
    "    # 본문\n",
    "    content = article.get('content', {})\n",
    "\n",
    "    # 사진\n",
    "    imageList = article.get('imageList', {})\n",
    "\n",
    "    # 첫번째 사진 url\n",
    "    mainImgUrl = imageList[0].get('url') if imageList else None\n",
    "\n",
    "    # 첫번째 사진 캡션\n",
    "    mainImgCaption = imageList[0].get('caption') if imageList else None\n",
    "\n",
    "    # 관련 기사 리스트\n",
    "    relatedArticleList = article.get('relatedArticleList', {})\n",
    "    \n",
    "    news_info = {\n",
    "        '대분류': section, \n",
    "        '중분류': subSection, \n",
    "        '제목': title, \n",
    "        '작성시간': createDate, \n",
    "        '썸네일 이미지': mainImgUrl, \n",
    "        '썸네일 캡션': mainImgCaption, \n",
    "        '본문': content, \n",
    "        '기사주소': url\n",
    "    }\n",
    "    time.sleep(random.uniform(1, 2))\n",
    "    for i, relatedArticle in enumerate(relatedArticleList):\n",
    "        relatedArticleTitle = '관련기사 제목' + str(i + 1)\n",
    "        relatedArticleUrl = '관련기사 주소' + str(i + 1)\n",
    "        relatedArticleImgUrl = '관련기사 이미지' + str(i + 1)\n",
    "        news_info[relatedArticleTitle] = relatedArticle.get('title')\n",
    "        news_info[relatedArticleUrl] = 'https://www.hani.co.kr' + relatedArticle.get('url')\n",
    "        news_info[relatedArticleImgUrl] = relatedArticle.get('image', {}).get('url')\n",
    "    return news_info\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86033b51-bc21-4184-8205-4bc7209ba134",
   "metadata": {},
   "source": [
    "1. 필요한 라이브러리 임포트\n",
    "-> 없으면 해당 라이브러리 pip install {추가해야하는 라이브러리 입력}\n",
    "2. 두번째 블럭 csv 파일을 저장할 디렉토리 설정\n",
    "-> os.chdir(r'C:\\Users\\SSAFY\\Desktop\\crawling\\hani')\n",
    "-> 해당 디렉토리에 csv 파일 저장\n",
    "3. 맨 처음 블럭부터 ctrl + enter (notebook jupyter 사용하는 경우)\n",
    "4. 두번째 블록의 카테고리 인덱스 확인하고 아래 #### 변수 값 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8bb7345-923e-47b4-a785-84d9b6c8922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 크롤링 시작점\n",
    "# 1페이지에서 10페이지 전체 기사 출력하기\n",
    "from datetime import datetime\n",
    "\n",
    "##### 크롤링시 변경해야하는 값\n",
    "start_page = 1 # 시작페이지\n",
    "end_page = 10 # 크롤링 마지막 페이지\n",
    "category_idx = 3 # 분야 카테고리  # 0:정치, 1:사회, 2:전국, 3:경제, 국제, 문화, 스포츠, 미래과학, 애니멀피플, 오피니언\n",
    "#####\n",
    "\n",
    "page = start_page\n",
    "while page < end_page:\n",
    "    try:\n",
    "        article_data = []\n",
    "        url = make_hani_url(category_idx, page)  # 경제 카테고리의 1페이지\n",
    "        links = extract_hani_article_link(url)  # 1페이지 기사의 모든 url\n",
    "        for link in links:\n",
    "            news_info = extract_article_info(link)\n",
    "            article_data.append(news_info)\n",
    "        df = pd.DataFrame(article_data)\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        file_name = f'article_{current_time}.csv'\n",
    "        df.to_csv(file_name, index=False, encoding='utf-8')\n",
    "        time.sleep(random.uniform(0, 1))\n",
    "        page += 1\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        page -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594f2fc-1f22-4785-b9b8-8df5288b0770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
