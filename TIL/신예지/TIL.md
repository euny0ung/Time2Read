# TIL
## 24.02.27
### 한 일
- 아이디어 선정 회의
- 아이디어 구체화
- 컨설턴트님과 코치님과의 컨설팅
- Hadoop Partitioner & Inverted Index 이론 학습과 실습 연습
- 뉴스 데이터를 분석 처리 할 수 있는 방법 모색
     

### 배운 것 
- [Hadoop]
  - Partitioner : Map 함수의 출력인 (key, value)쌍이 Key에 대한 해시 값에 따라 어느 Reducer(머신)으로 보내질 것인지 결정을 정의하는 class이다.
    - 리듀스를 여러개 사용하여 분산 처리하는 만큼 어떤 데이터를 어디로 보낼지 통일 시키여야 하는데 그 정의를 Partitioner를 상속받아 오버라이딩 할 수 있다.
  - Inverted Index : 각 단어가 어느 문서에서 어떤 위치에 나오는지를 리스트로 출력받을 수 있다. 
- [뉴스 데이터를 사용한 분석 처리 방법과 예시]
    - 텍스트 분석 : 기사 텍스트를 분석해 기사 내 감정을 분석하여 긍정적, 부정적, 중립적을 판단할 수 있다. 
        - 텍스트 전처리 : python의 Natural Language Toolkit(NLTK), spaCy, KoNLPy 라이브러리 사용
        - 텍스트 분석 : TF-IDF, Word2Vec, Doc2Vec 등의 알고리즘을 사용하여 텍스트를 벡터로 변환하고, 클러스터링이나 감성 분석
    - 토픽 모델링, 토픽에 따른 분류 처리 : LDA(Latent Dirichlet Allocation), NMF(Non-negative Matrix Factorization)라는 토픽 모델링 알고리즘이 있다.
    - 연관 분석 : 뉴스 기간 간 연관성을 찾을 수 있다. -> 유사한 주제를 다루는 뉴스 기사를 클러스터링 
        - Apriori, FP-Growth 이라는 연관 규칙 학습 알고리즘 있어 연관 규칙을 찾을 수 있다.
        - 분산 처리 시스템인 Apache Spark를 사용해 대용량 데이터에 대한 연관 분석을 수행할 수 있다.
    - 데이터 시각화 : word cloud, heatmap을 사용해 사용자에게 데이터 패턴이나 트렌드를 시각화하여 제공할 수 있다.
        - Matplotlib, Seaborn, Plotly라는 그래프 같이 시각적 정보를 제공해주는 라이브러리가 존재한다.
    - 실시간 분석 : 실시간으로 뉴스 데이터를 분석하여 트랜드를 파악하거나 중요 사건을 탐지 할 수 있다.
        - Apache Kafka로 실시간 데이터를 수집하고 처리할 수 있다.
        - 실시간 처리 프레임워크로 Apache Flink, Apache Storm가 있다.
    - 정보 검색과 추천 : 사용자에게 뉴스를 추천하거나 주제에 대한 정보를 검색할 수 있다.
        - Elasticsearch, Apache Solr와 같은 검색 엔진이 존재한다.
        - 추천 알고리즘을 직접 구현하는 것도 방법이다.

### 아직 잘 모르는 것
- 뉴스 데이터를 사용한 분석 처리 방법과 예시에서 사용되는 기술에 대해 정확한 정의와 기술에 대해 명확한 지식이 필요하다.

---

## 24.02.27
### 한 일
- 아이디어 회의
- 최종 아이디어 구체화
- MapReduce 정리
### 배운 것 
- 분산 처리를 하는 이유
  Scale-out은 아주 많은 값싼 서버들을 이용하는 것이고, Scale-up 적은 수의 값비싼 서버들을 이용하는 것이다.

  데이터 중심 어플리케이션 분야에서는 Scale-out을 선호한다.
  왜냐면, 고가의 서버들은 가격 관점에서는 선형으로 성능이 증가하지 않기 때문이다. 즉, 2배 성능 프로세스 한 개가 일반 프로세서 한 개의 컴퓨터 가격의 2배 보다 훨씬 비싸다는 것이다.

- MapReduce란?
  맵리듀스는 여러 노드에 태스크를 분배하는 방법으로 Map과 Reduce라는 두 단계로 구성되어 처리된다. 이 Map과 Reduce는 각각의 분산 처리 장소에서 각각은 직렬적으로 처리되고 분산되었기 때문에 크게 보아 병렬적으로 수행된다고 할 수 있다.

  값싼 컴퓨터들을 모아 클러스터를 만들고 빅데이터를 처리하기 위한 스케일러블 병렬 소프트웨어의 구현을 쉽게 할 수 있도록 도와주인 프로그램 모델이다.

  Google의 MapReduce 또는 오픈소스 Hadoop은 MapReduce 프레임워크의 구현의 형태이다.

- 클러스터?
  클러스터(Cluster)란 여러 컴퓨터 또는 서버가 네트워크를 통해 연결되어 하나의 시스템으로 동작하는 컴퓨팅 리소스의 집합

- 스케일러블 (scalable)?
  사용자 수나 데이터가 급증해도 프로그램이 멈추거나 성능이 크게 떨어지는 일이 없다는 뜻

- 왜 MapReduce를 사용해야 할까?
  데이터 중심 프로세싱은 한대의 컴퓨터 능력으로는 처리가 어려워 수십, 수백, 수천대의 컴퓨터를 묶어서 분산하여 처리해야한다. 이때 MapReduce 프레임워크가 이 일을한다.
  존재하는 여러 가지 다른 병렬 컴퓨팅 방법에서는 개발자가 low level의 시스템까지 잘 알아야 하고 많은 시간이 소요되지만 MapReduce 보다 접근이 쉬우며 빅데이터의 효율적인 계산이 가능한 방법 중 하나이다.

- MapReduce 동작 방식
  드라이버에 해당하는 메인 함수가 map함수와 reduce함수를 호출해서 처리한다.
  맵리듀스에서 각각의 레코드는 key-value 쌍으로 표현된다.

- Map 함수가 하는 역할
  (key1, value1) -> [(key2, value2)]

- Reduce함수가 하는 역할
  (key2, [value2, value3, ...] ) -> [(key3, value4)]

나의 velog에 정리한 내용 https://velog.io/@shinyeji28/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-MapReduce

### 아직 잘 모르는 것
분산 처리 시 실습으로 하나의 컴퓨터로 수행할 때 thread를 머신이라 두고 분산처리 하는 것인지 알아봐야 한다.